% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/htlr.R
\name{htlr}
\alias{htlr}
\title{Fit a HTLR Model}
\usage{
htlr(X, y, fsel = 1:ncol(x), stdzx = TRUE, prior = c("t", "neg",
  "ghs"), iter = 2000, warmup = floor(iter/2), thin = 1,
  init = "lasso", leap = 50, leap.warm = floor(iter/10),
  leap.step = 0.3, sgmcut = 0.05, alpha = 1, logw = -10, eta = 0,
  sigmab0 = 2000, verbose = FALSE, pre.legacy = FALSE, ...)
}
\arguments{
\item{X}{Design matrix of traning data; 
rows should be for the cases, and columns for different features.}

\item{y}{Vector of class labels in training or test data set. 
Must be coded as non-negative integers, e.g., 1,2,\ldots,C for C classes.}

\item{fsel}{Subsets of features selected before fitting, such as by univariate screening.}

\item{stdzx}{Logical; if \code{TRUE}, the original feature values are standardized to have \code{mean} = 0 
and \code{sd} = 1.}

\item{prior}{The prior to be applied to the model. Either "t" (default), "ghs" (horseshoe), 
or "neg" (normal-exponential-gamma).}

\item{iter}{A positive integer specifying the number of iterations (including warmup).}

\item{warmup}{A positive integer specifying the number of warmup (aka burnin). 
The number of warmup iterations should not be larger than iter and the default is iter/2.}

\item{thin}{A positive integer specifying the period for saving samples.}

\item{init}{The initial state of Markov Chain; can be NULL, 
or a gived parameter vector, or a previous markov chain results.}

\item{leap}{The length of leapfrog trajectory in sampling phase.}

\item{leap.warm}{The length of leapfrog trajectory in burnin phase.}

\item{leap.step}{The stepsize adjustment multiplied to the second-order partial derivatives of log posterior.}

\item{alpha}{The degree freedom of t/ghs/neg prior for coefficients.}

\item{logw}{The log scale of priors for coefficients.}

\item{eta}{The \code{sd} of the normal prior for logw. When it is set to 0, logw is fixed. 
Otherwise, logw is assigned with a normal prior and it will be updated during sampling.}

\item{sigmab0}{The \code{sd} of the normal prior for the intercept.}

\item{verbose}{Logical; setting it to \code{TRUE} for tracking MCMC sampling iterations.}

\item{pre.legacy}{Logical; if \code{TRUE}, the output produced in \code{HTLR} versions up to 
legacy-3.1-1 is reproduced.}

\item{...}{Other optional parameters:
\itemize{
  \item alpha.rda - A user supplied alpha value for \code{bcbcsf_deltas}. Default: 0.2.
  \item lasso.lambda - A user supplied lambda value for \code{lasso_deltas}. Default: 0.01.
  Will be ignored if .legacy is set to TRUE. 
}}

\item{cut}{The coefficients smaller than this criteria will be fixed in each HMC updating step.}
}
\value{
A list of fitting results.
}
\description{
This function trains linear logistic regression models with HMC in restricted Gibbs sampling.
}
\references{
Longhai Li and Weixin Yao. (2018). Fully Bayesian Logistic Regression 
with Hyper-Lasso Priors for High-dimensional Feature Selection.
\emph{Journal of Statistical Computation and Simulation} 2018, 88:14, 2827-2851.
}
