% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bplrhmc.r
\name{htlr_fit}
\alias{htlr_fit}
\title{Fit a HTLR models and make predictions}
\usage{
htlr_fit(y_tr, X_tr, X_ts = NULL, fsel = 1:ncol(X_tr), stdzx = TRUE,
  sigmab0 = 2000, ptype = "t", alpha = 1, s = -10, eta = 0,
  iters_h = 1000, iters_rmc = 1000, thin = 1, leap_L = 50,
  leap_L_h = 5, leap_step = 0.3, hmc_sgmcut = 0.05,
  initial_state = "lasso", alpha.rda = 0.2, silence = TRUE,
  pre.legacy = TRUE, predburn = NULL, predthin = 1, ...)
}
\arguments{
\item{y_tr}{Vector of class labels in training or test data set. 
Must be coded as non-negative integers, e.g., 1,2,\ldots,C for C classes.}

\item{X_tr}{Design matrix of traning data; 
rows should be for the cases, and columns for different features.}

\item{fsel}{Subsets of features selected before fitting, such as by univariate screening.}

\item{stdzx}{Logical; if \code{TRUE}, the original feature values are standardized to have \code{mean} = 0 
and \code{sd} = 1.}

\item{sigmab0}{The \code{sd} of the normal prior for the intercept.}

\item{ptype}{The prior to be applied to the model. Either "t" (student-t, default), "ghs" (horseshoe), 
or "neg" (normal-exponential-gamma).}

\item{alpha}{The degree freedom of t/ghs/neg prior for coefficients.}

\item{s}{The log scale of priors (logw) for coefficients.}

\item{eta}{The \code{sd} of the normal prior for logw. When it is set to 0, logw is fixed. 
Otherwise, logw is assigned with a normal prior and it will be updated during sampling.}

\item{iters_rmc}{A positive integer specifying the number of iterations after warmup.}

\item{thin}{A positive integer specifying the period for saving samples.}

\item{leap_L}{The length of leapfrog trajectory in sampling phase.}

\item{leap_L_h}{The length of leapfrog trajectory in burnin phase.}

\item{leap_step}{The stepsize adjustment multiplied to the second-order partial derivatives of log posterior.}

\item{hmc_sgmcut}{The coefficients smaller than this criteria will be fixed in each HMC updating step.}

\item{initial_state}{The initial state of Markov Chain; can be NULL, 
or a gived parameter vector, or a previous markov chain results.}

\item{silence}{Setting it to \code{FALSE} for tracking MCMC sampling iterations.}

\item{pre.legacy}{Logical; if \code{TRUE}, the output produced in \code{HTLR} versions up to 
legacy-3.1-1 is reproduced.}

\item{...}{Other optional parameters:
\itemize{
  \item alpha.rda - A user supplied alpha value for \code{bcbcsf_deltas}. Default: 0.2.
  \item lasso.lambda - A user supplied lambda sequence for \code{lasso_deltas}. 
  Default: {.01, .02, \ldots, .05}. Will be ignored if .legacy is set to TRUE.
}}

\item{iter_h}{A positive integer specifying the number of warmup (aka burnin).}
}
\value{
A list of fitting results.
}
\description{
This function trains linear logistic regression models with HMC in restricted Gibbs sampling. 
It also makes predictions for test cases if \code{X_ts} are provided.
}
\examples{
\dontrun{
data (gen_grpcor_data)
## creating training and test data sets
tr <- 200
X_tr <- data$X[1:tr,]
y_tr <- data$y[1:tr]
X_ts <- data$X[-(1:tr), ]
y_ts <- data$y[-(1:tr)]
## fit htlr models
fithtlr <- htlr_fitpred (
  y_tr = y_tr, X_tr = X_tr, stdzx = TRUE, #fsel = c (1,51,101),
  pty = "t", alpha = 1, s = -15, 
  iters_h = 1000, iters_rmc = 1000, thin = 50,
  leap_L_h = 5, leap_L = 50, leap_step = 0.5, hmc_sgmcut = 0.05, 
  initial_state = "bcbcsfrda", silence = F)
}

}
\references{
Longhai Li and Weixin Yao. (2018). Fully Bayesian Logistic Regression 
with Hyper-Lasso Priors for High-dimensional Feature Selection.
\emph{Journal of Statistical Computation and Simulation} 2018, 88:14, 2827-2851.
}
\seealso{
htlr
}
