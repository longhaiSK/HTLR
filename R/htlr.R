#' Fit a HTLR Model
#'
#' This function trains linear logistic regression models with HMC in restricted Gibbs sampling. 
#'
#' @param X Design matrix of traning data; 
#' rows should be for the cases, and columns for different features.
#' 
#' @param y Vector of class labels in training or test data set. 
#' Must be coded as non-negative integers, e.g., 1,2,\ldots,C for C classes.
#' 
#' @param fsel Subsets of features selected before fitting, such as by univariate screening.
#' @param stdzx Logical; if \code{TRUE}, the original feature values are standardized to have \code{mean} = 0 
#' and \code{sd} = 1.
#' 
#' @param iter A positive integer specifying the number of iterations (including warmup).
#' @param warmup A positive integer specifying the number of warmup (aka burnin). 
#' The number of warmup iterations should not be larger than iter and the default is iter/2.
#' 
#' @param thin A positive integer specifying the period for saving samples.
#' 
#' @param leap The length of leapfrog trajectory in sampling phase.
#' @param leap.warm The length of leapfrog trajectory in burnin phase.
#' @param leap.step The stepsize adjustment multiplied to the second-order partial derivatives of log posterior.
#' 
#' @param cut The coefficients smaller than this criteria will be fixed in each HMC updating step.
#' 
#' @param init The initial state of Markov Chain; can be a previously fitted \code{fithtlr} object, 
#' or a user supplied initial state vector, or a character string matches the following:  
#' \itemize{
#'   \item "lasso" - (Default) Use Lasso initial state with \code{lambda} chosen by 
#'   cross-validation. Users may specify their own candidate \code{lambda} values via 
#'   optional argument \code{lasso.lambda}. Further customized Lasso initial 
#'   states can be generated by \code{\link{lasso_deltas}}.    
#'   \item "bcbc" - Use initial state generated by package \code{BCBCSF} 
#'   (Bias-corrected Bayesian classification). Further customized BCBCSF initial 
#'   states can be generated by \code{\link{bcbcsf_deltas}}. WARNING: This type of 
#'   initial states can be used for continuous features such as gene expression profiles, 
#'   but it should not be used for categorical features such as SNP profiles.
#'   \item "random" - Use random initial values sampled from N(0, 1).     
#' }
#' 
#' @param prior The prior to be applied to the model. Either a list of hyperparameter settings 
#' returned by \code{\link{htlr_prior}} or a character string from "t" (student-t), "ghs" (horseshoe), 
#' and "neg" (normal-exponential-gamma).
#' 
#' @param df The degree freedom of t/ghs/neg prior for coefficients. Will be ignored if the 
#' configuration list from \code{\link{htlr_prior}} is passed to \code{prior}. 
#' 
#' @param verbose Logical; setting it to \code{TRUE} for tracking MCMC sampling iterations.
#' 
#' @param pre.legacy Logical; if \code{TRUE}, the output produced in \code{HTLR} versions up to 
#' legacy-3.1-1 is reproduced. The speed would be typically slower than non-legacy mode on
#' multi-core machine. 
#' 
#' @param ... Other optional parameters:
#' \itemize{
#'   \item rda.alpha - A user supplied alpha value for \code{\link{bcbcsf_deltas}}. Default: 0.2.
#'   \item lasso.lambda - A user supplied lambda sequence for \code{\link{lasso_deltas}}. 
#'   Default: \{.01, .02, \ldots, .05\}. Will be ignored if \code{pre.legacy} is set to \code{TRUE}.
#' } 
#' 
#' @return An object with S3 class \code{htlrfit}.  
#' 
#' @references
#' Longhai Li and Weixin Yao (2018). Fully Bayesian Logistic Regression 
#' with Hyper-Lasso Priors for High-dimensional Feature Selection.
#' \emph{Journal of Statistical Computation and Simulation} 2018, 88:14, 2827-2851.
#' 
#' @useDynLib HTLR
#' 
#' @import Rcpp stats
#' 
#' @export
#' 
#' @examples
#' set.seed(12345)
#' data("colon")
#' 
#' ## fit HTLR models with selected features, note that the chain length setting is for demo only
#'
#' ## using t prior with 1 df and log-scale ~ N(-10, 10) 
#' fit.t <- htlr(X = colon$X, y = colon$y, fsel = 1:100,
#'               prior = htlr_prior("t", df = 1, logw = -10, eta = 10), 
#'               init = "bcbc", iter = 20, thin = 1)
#'
#' ## using NEG prior with 1 df and log-scale fixed to -10 
#' fit.neg <- htlr(X = colon$X, y = colon$y, fsel = 1:100,
#'                 prior = htlr_prior("neg", df = 1, logw = -10), 
#'                 init = "bcbc", iter = 20, thin = 1)
#'
#' ## using horseshoe prior with 1 df and auto-selected log-scale   
#' fit.ghs <- htlr(X = colon$X, y = colon$y, fsel = 1:100,
#'                 prior = "ghs", df = 1, init = "bcbc",
#'                 iter = 20, thin = 1)
#'
#' @seealso htlr_prior
#' 
htlr <-
  function (X, y,
            fsel = 1:ncol(X),
            stdzx = TRUE,
            prior = "t",
            df = 1,
            iter = 2000,
            warmup = floor(iter/2), 
            thin = 1,
            init = "lasso",
            leap = 50,
            leap.warm = floor(leap/10),
            leap.step = 0.3,
            cut = 0.05,
            verbose = FALSE,
            pre.legacy = FALSE,
            ...
  )
{
  #------------------------------- Input Checking -------------------------------#
  
  if (length (y) != nrow (X) ) 
    stop ("'y' and 'X' mismatch")
  
  yfreq <- table(y)
  if (length(yfreq) < 2)
    stop("less than 2 classes of response")
  if (any(yfreq < 2)) 
    stop("less than 2 cases in some group")
  
  if (is.character(prior))
  {
    prior <- htlr_prior(prior, df)
  }
  
  ptype <- prior$ptype
  alpha <- prior$alpha
  logw <- prior$logw
  eta <- prior$eta
  sigmab0 <- prior$sigmab0
  
  stopifnot(iter > warmup, warmup > 0, thin > 0, leap > 0, leap.warm > 0,
            alpha > 0, eta >= 0, sigmab0 >= 0)
  
  #----------------------------- Data preprocessing -----------------------------#
  
  if (min(y) == 0) y <- y + 1
  
  ybase <- as.integer(y - 1)
  ymat <- model.matrix( ~ factor(y) - 1)[, -1]
  C <- length(unique(ybase))
  K <- C - 1
  
  ## feature selection
  X <- X[, fsel, drop = FALSE]
  p <- length(fsel)
  n <- nrow(X)
  
  ## standardize selected features
  nuj <- rep(0, length(fsel))
  sdj <- rep(1, length(fsel))
  if (stdzx == TRUE & !is.numeric(init))
  {
    nuj <- apply(X, 2, median)
    sdj <- apply(X, 2, sd)
    X <- sweep(X, 2, nuj, "-")
    X <- sweep(X, 2, sdj, "/")
  }
  
  ## add intercept
  X.addint <- cbind(1, X)
  
  ## stepsize for HMC from data
  DDNloglike <- 1 / 4 * colSums(X.addint ^ 2)
  
  #---------------------- Markov chain state initialization ----------------------#

  if (is.list(init)) # use the last iteration of markov chain
  {
    no.mcspl <- length(init$mclogw)
    deltas <- matrix(init$mcdeltas[, , no.mcspl], nrow = p + 1)
    sigmasbt <- init$mcsigmasbt[, no.mcspl]
    logw <- init$mclogw[no.mcspl]
  }
  else
  {
    if (is.matrix(init)) # user supplied deltas
    {
      deltas <- init
      if (nrow (deltas) != p + 1 || ncol(deltas) != K)
      {
        stop(
          sprintf(
            "Initial `deltas' mismatch data. Expected: nrow=%d, ncol=%d; Actual: nrow=%d, ncol=%d.",
            p + 1, K, nrow(deltas), ncol(deltas))
        )        
      }
    }
    else if (init == "lasso")
    {
      if (pre.legacy) 
        lasso.lambda <- NULL # will be chosen by CV
      else if (!exists("lasso.lambda"))
        lasso.lambda <- seq(.05, .01, by = -.01)
      # else lambda is supplied via optional arg
      deltas <- lasso_deltas(X, y, lasso.lambda, verbose)
    }
    else if (init == "bcbc")
    {
      if (!exists("rda.alpha"))
        alpha.rda <- .2
      deltas <- bcbcsf_deltas(X, y, alpha.rda)
    }
    else if (init == "random")
    {
      deltas <- matrix(rnorm((p + 1) * K) * 2, p + 1, K)
    }
    else stop("not supported init type")
    
    vardeltas <- comp_vardeltas(deltas)[-1]
    sigmasbt <- c(sigmab0, spl_sgm_ig(alpha, K, exp(logw), vardeltas))
  }
  
  #-------------------------- Do Gibbs sampling --------------------------#
  
  fit <- HtlrFitHelper(
    ## data
    p = p, K = K, n = n,
    X = as.matrix(X.addint), 
    ymat = as.matrix(ymat), 
    ybase = as.vector(ybase),
    ## prior
    ptype = ptype, alpha = alpha, s = logw, eta = eta,
    ## sampling
    iters_rmc = (iter - warmup), iters_h = warmup, thin = thin, 
    leap_L = leap, leap_L_h = leap.warm, leap_step = leap.step, 
    hmc_sgmcut = cut, DDNloglike = as.vector(DDNloglike),
    ## fit result
    deltas = deltas, logw = logw, sigmasbt = sigmasbt,
    ## other control
    silence = as.integer(!verbose), legacy = pre.legacy)
  
  # add prior hyperparameter information
  fit$prior <- prior
  
  # add data preprocessing information
  fit$feature <- list("fsel" = fsel, "nuj" = nuj, "sdj" = sdj, "y" = y)
  
  # register S3
  attr(fit, "class") <- "htlrfit"
  return(fit)
}