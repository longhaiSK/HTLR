[{"path":"https://longhaisk.github.io/HTLR/articles/simu.html","id":"data-generation","dir":"Articles","previous_headings":"","what":"Data Generation","title":"Multinomial Logistic Regression with Heavy-Tailed Priors","text":"Load necessary libraries: description dataset generating scheme found Li Yao (2018). 4 groups features: feature #1: marginally related feature feature #2: marginally unrelated feature, feature #2 correlated feature #1 feature #3 - #10: marginally related features also internally correlated feature #11 - #2000: noise features without relationship y Look correlation features:  Split data training testing sets:","code":"library(HTLR) library(bayesplot) #> This is bayesplot version 1.11.1 #> - Online documentation and vignettes at mc-stan.org/bayesplot #> - bayesplot theme set to bayesplot::theme_default() #>    * Does _not_ affect other ggplot2 plots #>    * See ?bayesplot_theme_set for details on theme setting SEED <- 123  n <- 510 p <- 2000  means <- rbind(   c(0, 1, 0),   c(0, 0, 0),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1) ) * 2  means <- rbind(means, matrix(0, p - 10, 3))  A <- diag(1, p)  A[1:10, 1:3] <-   rbind(     c(1, 0, 0),     c(2, 1, 0),     c(0, 0, 1),     c(0, 0, 1),     c(0, 0, 1),     c(0, 0, 1),     c(0, 0, 1),     c(0, 0, 1),     c(0, 0, 1),     c(0, 0, 1)   )  set.seed(SEED) dat <- gendata_FAM(n, means, A, sd_g = 0.5, stdx = TRUE) str(dat) #> List of 4 #>  $ X  : num [1:510, 1:2000] -0.684 0.912 -0.997 -1.262 0.613 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:2000] \"V1\" \"V2\" \"V3\" \"V4\" ... #>  $ muj: num [1:2000, 1:3] -0.456 0 -0.456 -0.376 -0.376 ... #>  $ SGM: num [1:2000, 1:2000] 0.584 0.597 0 0 0 ... #>  $ y  : int [1:510] 1 2 3 1 2 3 1 2 3 1 ... # require(corrplot) cor(dat$X[ , 1:11]) %>% corrplot::corrplot(tl.pos = \"n\") set.seed(SEED) dat <- split_data(dat$X, dat$y, n.train = 500) str(dat) #> List of 4 #>  $ x.tr: num [1:500, 1:2000] -0.903 -0.632 1.111 1.446 -0.43 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:2000] \"V1\" \"V2\" \"V3\" \"V4\" ... #>  $ y.tr: int [1:500] 1 1 2 2 3 3 3 1 2 1 ... #>  $ x.te: num [1:10, 1:2000] 1.955 1.188 -0.942 -1.387 0.879 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:2000] \"V1\" \"V2\" \"V3\" \"V4\" ... #>  $ y.te: int [1:10] 2 2 3 3 2 3 1 2 2 2"},{"path":"https://longhaisk.github.io/HTLR/articles/simu.html","id":"model-fitting","dir":"Articles","previous_headings":"","what":"Model Fitting","title":"Multinomial Logistic Regression with Heavy-Tailed Priors","text":"Fit HTLR model default settings: another configuration:","code":"set.seed(SEED) system.time(   fit.t <- htlr(dat$x.tr, dat$y.tr) ) #>    user  system elapsed  #> 231.912   0.101  58.808 print(fit.t) #> Fitted HTLR model  #>  #>  Data: #>  #>   response:  3-class #>   observations:  500 #>   predictors:    2001 (w/ intercept) #>   standardised:  TRUE  #>  #>  Model: #>  #>   prior dist:    t (df = 1, log(w) = -10.0) #>   init state:    lasso  #>   burn-in:   1000 #>   sample:    1000 (posterior sample size)  #>  #>  Estimates: #>  #>   model size:    4 (w/ intercept) #>   coefficients: see help('summary.htlr.fit') set.seed(SEED) system.time(   fit.t2 <- htlr(X = dat$x.tr, y = dat$y.tr,                   prior = htlr_prior(\"t\", df = 1, logw = -20, sigmab0 = 1500),                   iter = 4000, init = \"bcbc\", keep.warmup.hist = T) ) #>    user  system elapsed  #> 358.746   0.587  91.719 print(fit.t2) #> Fitted HTLR model  #>  #>  Data: #>  #>   response:  3-class #>   observations:  500 #>   predictors:    2001 (w/ intercept) #>   standardised:  TRUE  #>  #>  Model: #>  #>   prior dist:    t (df = 1, log(w) = -20.0) #>   init state:    bcbc  #>   burn-in:   2000 #>   sample:    2000 (posterior sample size)  #>  #>  Estimates: #>  #>   model size:    4 (w/ intercept) #>   coefficients: see help('summary.htlr.fit')"},{"path":"https://longhaisk.github.io/HTLR/articles/simu.html","id":"model-inspection","dir":"Articles","previous_headings":"","what":"Model Inspection","title":"Multinomial Logistic Regression with Heavy-Tailed Priors","text":"Look point summaries posterior selected parameters: Plot interval estimates posterior draws using bayesplot:  Trace plot MCMC draws:  coefficient unrelated features (noise) updated iterations due restricted Gibbs sampling Li Yao (2018), hence computational cost greatly reduced.","code":"summary(fit.t2, features = c(1:10, 100, 200, 1000, 2000), method = median) #>                 class 2       class 3 #> Intercept -2.7354013973 -1.119076e+00 #> V1         8.2082272603 -6.028105e-01 #> V2        -4.5701096071  2.313064e-01 #> V3        -0.9363523582  3.214379e+00 #> V4         0.0009046690 -2.152431e-03 #> V5        -0.0051665924 -5.631749e-05 #> V6        -0.0072774286  1.654051e-03 #> V7        -0.0013794333 -2.153372e-03 #> V8        -0.0047661861 -7.569987e-03 #> V9        -0.0060317708  2.927544e-04 #> V10       -0.0005934682  1.050192e-02 #> V100      -0.0039015133  1.115288e-02 #> V200      -0.0066909487  6.128469e-04 #> V1000      0.0051105291  6.717532e-03 #> V2000     -0.0058668522 -7.001962e-03 #> attr(,\"stats\") #> [1] \"median\" post.t <- as.matrix(fit.t2, k = 2) ## signal parameters mcmc_intervals(post.t, pars = c(\"Intercept\", \"V1\", \"V2\", \"V3\", \"V1000\")) as.matrix(fit.t2, k = 2, include.warmup = T) %>%   mcmc_trace(c(\"V1\", \"V1000\"), facet_args = list(\"nrow\" = 2), n_warmup = 2000)"},{"path":"https://longhaisk.github.io/HTLR/articles/simu.html","id":"make-predictions","dir":"Articles","previous_headings":"","what":"Make Predictions","title":"Multinomial Logistic Regression with Heavy-Tailed Priors","text":"glance prediction accuracy: details prediction result:","code":"y.class <- predict(fit.t, dat$x.te, type = \"class\") y.class #>       y.pred #>  [1,]      2 #>  [2,]      2 #>  [3,]      3 #>  [4,]      3 #>  [5,]      2 #>  [6,]      3 #>  [7,]      3 #>  [8,]      2 #>  [9,]      2 #> [10,]      2 print(paste0(\"prediction accuracy of model 1 = \",               sum(y.class == dat$y.te) / length(y.class))) #> [1] \"prediction accuracy of model 1 = 0.9\"  y.class2 <- predict(fit.t2, dat$x.te, type = \"class\") print(paste0(\"prediction accuracy of model 2 = \",               sum(y.class2 == dat$y.te) / length(y.class))) #> [1] \"prediction accuracy of model 2 = 0.9\" predict(fit.t, dat$x.te, type = \"response\") %>%   evaluate_pred(y.true = dat$y.te) #> $prob_at_truelabels #>  [1] 0.9993980 0.9996598 0.9960531 0.9146809 0.5601952 0.4681349 0.1604013 #>  [8] 0.9999976 0.9986511 0.9655200 #>  #> $table_eval #>    Case ID True Label Pred. Prob 1 Pred. Prob 2 Pred. Prob 3 Wrong? #> 1        1          2 2.230163e-04 9.993980e-01 3.789681e-04      0 #> 2        2          2 3.397451e-04 9.996598e-01 4.055238e-07      0 #> 3        3          3 3.946910e-03 3.274933e-09 9.960531e-01      0 #> 4        4          3 8.531913e-02 1.201749e-08 9.146809e-01      0 #> 5        5          2 4.363766e-01 5.601952e-01 3.428135e-03      0 #> 6        6          3 1.231557e-01 4.087093e-01 4.681349e-01      0 #> 7        7          1 1.604013e-01 9.352412e-04 8.386635e-01      1 #> 8        8          2 2.383022e-06 9.999976e-01 3.584158e-08      0 #> 9        9          2 1.339940e-03 9.986511e-01 8.967895e-06      0 #> 10      10          2 3.263762e-02 9.655200e-01 1.842378e-03      0 #>  #> $amlp #> [1] 0.3299063 #>  #> $err_rate #> [1] 0.1 #>  #> $which.wrong #> [1] 7"},{"path":"https://longhaisk.github.io/HTLR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Longhai Li. Author, maintainer. Steven Liu. Author.","code":""},{"path":"https://longhaisk.github.io/HTLR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Li, L., & Yao, W. (2018). Fully Bayesian logistic regression hyper-LASSO priors high-dimensional feature selection. Journal Statistical Computation Simulation, 88(14), 2827-2851.","code":"@Article{,   title = {Fully Bayesian logistic regression with hyper-LASSO priors for high-dimensional feature selection},   journal = {Journal of Statistical Computation and Simulation},   year = {2018},   author = {Longhai Li and Weixin Yao},   volume = {88},   number = {14},   pages = {2827--2851},   publisher = {Taylor & Francis}, }"},{"path":"https://longhaisk.github.io/HTLR/index.html","id":"htlr-bayesian-logistic-regression-with-heavy-tailed-priors","dir":"","previous_headings":"","what":"HTLR: Bayesian Logistic Regression with Heavy-tailed Priors","title":"Bayesian Logistic Regression with Heavy-Tailed Priors","text":"HTLR performs classification feature selection fitting Bayesian polychotomous (multiclass, multinomial) logistic regression models based heavy-tailed priors small degree freedom. package suitable classification high-dimensional features, gene expression profiles. Heavy-tailed priors can impose stronger shrinkage (compared Guassian Laplace priors) coefficients associated large number useless features, still allow coefficients small number useful features stand little punishment. Heavy-tailed priors can also automatically make selection within large number correlated features. posterior coefficients hyperparameters sampled resitricted Gibbs sampling leveraging high-dimensionality Hamiltonian Monte Carlo handling high-correlations among coefficients.","code":""},{"path":"https://longhaisk.github.io/HTLR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Logistic Regression with Heavy-Tailed Priors","text":"CRAN version (recommended): Development version GitHub: package uses library Armadillo carrying matrix operations, may get speed benefits using alternative BLAS library ATLAS, OpenBLAS Intel MKL. Check post comparison installation guide. Windows users may consider installing Microsoft R Open.","code":"install.packages(\"HTLR\") # install.packages(\"devtools\") devtools::install_github(\"longhaiSK/HTLR\")"},{"path":"https://longhaisk.github.io/HTLR/index.html","id":"reference","dir":"","previous_headings":"","what":"Reference","title":"Bayesian Logistic Regression with Heavy-Tailed Priors","text":"Longhai Li Weixin Yao (2018). Fully Bayesian Logistic Regression Hyper-Lasso Priors High-dimensional Feature Selection. 2018, 88:14, 2827-2851, published version, arXiv version.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/HTLR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Logistic Regression with Heavy-Tailed Priors — HTLR-package","title":"Bayesian Logistic Regression with Heavy-Tailed Priors — HTLR-package","text":"Efficient Bayesian multinomial logistic regression based heavy-tailed priors. package suitable classification feature selection high-dimensional features, gene expression profiles. Heavy-tailed priors can impose stronger shrinkage (compared Gaussian Laplace priors) coefficients associated large number useless features, still allow coefficients small number useful features stand without punishment. can also automatically make selection within large number correlated features. posterior coefficients hyper- parameters sampled restricted Gibbs sampling leveraging high-dimensionality Hamiltonian Monte Carlo handling high-correlations among coefficients.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/HTLR-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian Logistic Regression with Heavy-Tailed Priors — HTLR-package","text":"Longhai Li Weixin Yao (2018). Fully Bayesian Logistic Regression Hyper-Lasso Priors High-dimensional Feature Selection. Journal Statistical Computation Simulation 2018, 88:14, 2827-2851.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/HTLR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bayesian Logistic Regression with Heavy-Tailed Priors — HTLR-package","text":"Maintainer: Longhai Li longhai@math.usask.ca (ORCID) Authors: Steven Liu xil865@usask.ca","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/as.matrix.htlr.fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Matrix of Markov Chain Samples — as.matrix.htlr.fit","title":"Create a Matrix of Markov Chain Samples — as.matrix.htlr.fit","text":"Markov chain samples (without warmup) included htlr.fit object coerced matrix.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/as.matrix.htlr.fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Matrix of Markov Chain Samples — as.matrix.htlr.fit","text":"","code":"# S3 method for class 'htlr.fit' as.matrix(x, k = NULL, include.warmup = FALSE, ...)"},{"path":"https://longhaisk.github.io/HTLR/reference/as.matrix.htlr.fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Matrix of Markov Chain Samples — as.matrix.htlr.fit","text":"x object S3 class htlr.fit. k Coefficients associated class k drawn. Must positive integer 1,2,...,C-1 C-class traning labels (base class 0 can chosen). default last class selected. binary logistic model argument can ignored. include.warmup Whether include warmup samples ... used.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/as.matrix.htlr.fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Matrix of Markov Chain Samples — as.matrix.htlr.fit","text":"matrix (p + 1) columns rows, p number features excluding intercept, number iterations burnin.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/as.matrix.htlr.fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Matrix of Markov Chain Samples — as.matrix.htlr.fit","text":"","code":"## No. of features used: 100; No. of iterations after burnin: 15  fit <- htlr(X = colon$X, y = colon$y, fsel = 1:100, iter = 20, warmup = 5)  dim(as.matrix(fit)) #> [1]  15 101"},{"path":"https://longhaisk.github.io/HTLR/reference/bcbcsf_deltas.html","id":null,"dir":"Reference","previous_headings":"","what":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","title":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","text":"Generate initial Markov chain state Bias-corrected Bayesian classification.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/bcbcsf_deltas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","text":"","code":"bcbcsf_deltas(X, y, alpha = 0)"},{"path":"https://longhaisk.github.io/HTLR/reference/bcbcsf_deltas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","text":"X Design matrix traning data; rows cases, columns different features. y Vector class labels training test data set. Must coded non-negative integers, e.g., 1,2,...,C C classes. alpha regularization proportion (0 1) mixing diagonal covariance estimates sample covariance estimated training samples. default 0, covariance matrix assumed diagonal, robust.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/bcbcsf_deltas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","text":"matrix - initial state Markov Chain HTLR model fitting.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/bcbcsf_deltas.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","text":"Caveat: method can used continuous predictors gene expression profiles, make sense categorical predictors SNP profiles.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/bcbcsf_deltas.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bias-corrected Bayesian classification initial state — bcbcsf_deltas","text":"Longhai Li (2012). Bias-corrected hierarchical Bayesian classification selected subset high-dimensional features. Journal American Statistical Association, 107(497), 120-134.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/colon.html","id":null,"dir":"Reference","previous_headings":"","what":"Colon Tissues — colon","title":"Colon Tissues — colon","text":"dataset, expression levels 40 tumor 22 normal colon tissues 6500 human genes measured using Affymetrix technology. selection 2000 genes highest minimal intensity across samples made Alon et al. (1999). data preprocessed carrying base 10 logarithmic transformation standardizing tissue sample zero mean unit variance across genes.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/colon.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Colon Tissues — colon","text":"","code":"data(\"colon\")"},{"path":"https://longhaisk.github.io/HTLR/reference/colon.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Colon Tissues — colon","text":"list contains data matrix X response vector y: X matrix 66 rows (observations) 2000 columns (features). y binary vector 0 indicates normal colon tissues 1 indicates tumor colon tissues.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/colon.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Colon Tissues — colon","text":"Dettling Marcel, Peter Bühlmann (2002). Supervised clustering genes. Genome biology, 3(12), research0069-1.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/diabetes392.html","id":null,"dir":"Reference","previous_headings":"","what":"Pima Indians Diabetes — diabetes392","title":"Pima Indians Diabetes — diabetes392","text":"dataset originally National Institute Diabetes Digestive Kidney Diseases. objective dataset diagnostically predict whether patient diabetes, based certain diagnostic measurements included dataset. Several constraints placed selection instances larger database. particular, patients females least 21 years old Pima Indian heritage. Different UCI original version, dataset preprocessed rows missing values removed, features scaled.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/diabetes392.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pima Indians Diabetes — diabetes392","text":"","code":"data(\"diabetes392\")"},{"path":"https://longhaisk.github.io/HTLR/reference/diabetes392.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Pima Indians Diabetes — diabetes392","text":"list contains data matrix X response vector y: X matrix 392 rows (observations) 8 columns (features). y binary vector 1 indicates diabetes patients 0 otherwise.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/diabetes392.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Pima Indians Diabetes — diabetes392","text":"https://www.kaggle.com/uciml/pima-indians-diabetes-database","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/diabetes392.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Pima Indians Diabetes — diabetes392","text":"Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using ADAP learning algorithm forecast onset diabetes mellitus. Proceedings Symposium Computer Applications Medical Care (pp. 261–265). IEEE Computer Society Press.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/evaluate_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate Prediction Results — evaluate_pred","title":"Evaluate Prediction Results — evaluate_pred","text":"function compares prediction results returned classifier ground truth, finally gives summary evaluation.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/evaluate_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate Prediction Results — evaluate_pred","text":"","code":"evaluate_pred(y.pred, y.true, caseid = names(y.true), showplot = TRUE, ...)"},{"path":"https://longhaisk.github.io/HTLR/reference/evaluate_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate Prediction Results — evaluate_pred","text":"y.pred matrix predicted probabilities, returned classifier. y.true Ground truth labels vector. caseid names test cases take account . default test cases used evaluation. showplot Logical; TRUE, summary plot generated. ... used.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/evaluate_pred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate Prediction Results — evaluate_pred","text":"summary evaluation result.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_FAM.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","title":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","text":"function generates inputs X given response variable y using multivariate normal model.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_FAM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","text":"","code":"gendata_FAM(n, muj, A, sd_g = 0, stdx = FALSE)"},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_FAM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","text":"n Number observations. muj C p matrix, row c representing y = c, column j representing \\(x_j\\). Used specify y. Factor loading matrix size p p, see details. sd_g Numeric value indicating noise level \\(\\delta\\), see details. stdx Logical; TRUE, data X standardized mean = 0 sd = 1.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_FAM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","text":"list contains input matrix X, response variables y, covariate matrix SGM muj (standardized stdx = TRUE).","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_FAM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","text":"means covariate \\(x_j\\) depend y specified matrix muj; covariate matrix \\(\\Sigma\\) multivariate normal equal \\(AA^t\\delta^2I\\), factor loading matrix \\(\\delta\\) noise level.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_FAM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Simulated Data with Factor Analysis Model — gendata_FAM","text":"","code":"## feature #1: marginally related feature ## feature #2: marginally unrelated feature, but feature #2 is correlated with feature #1 ## feature #3-5: marginally related features and also internally correlated ## feature #6-10: noise features without relationship with the y  set.seed(12345) n <- 100 p <- 10  means <- rbind(   c(0, 1, 0),   c(0, 0, 0),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1) ) * 2  means <- rbind(means, matrix(0, p - 5, 3))  A <- diag(1, p) A[1:5, 1:3] <- rbind(   c(1, 0, 0),   c(2, 1, 0),   c(0, 0, 1),   c(0, 0, 1),   c(0, 0, 1) )  dat <- gendata_FAM(n, means, A, sd_g = 0.5, stdx = TRUE) ggplot2::qplot(dat$y, bins = 6) #> Warning: `qplot()` was deprecated in ggplot2 3.4.0.  corrplot::corrplot(cor(dat$X))"},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_MLR.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Simulated Data with Multinomial Logistic Regression Model — gendata_MLR","title":"Generate Simulated Data with Multinomial Logistic Regression Model — gendata_MLR","text":"function generates response variables y given optional supplied X using multinomial logistic regression model.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_MLR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Simulated Data with Multinomial Logistic Regression Model — gendata_MLR","text":"","code":"gendata_MLR(n, p, NC = 3, nu = 2, w = 1, X = NULL, betas = NULL)"},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_MLR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Simulated Data with Multinomial Logistic Regression Model — gendata_MLR","text":"n Number observations. p Number features. NC Number classes response variables. nu, w betas supplied (default), regression coefficients generated t prior df = nu, scale = sqrt(w); ignored betas supplied. X design matrix; generated standard normal distribution supplied. betas User supplied regression coefficients.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_MLR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Simulated Data with Multinomial Logistic Regression Model — gendata_MLR","text":"list contains input matrix X, response variables y, regression coefficients deltas.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/gendata_MLR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Simulated Data with Multinomial Logistic Regression Model — gendata_MLR","text":"","code":"set.seed(12345) dat <- gendata_MLR(n = 100, p = 10) ggplot2::qplot(dat$y, bins = 6)  corrplot::corrplot(cor(dat$X))"},{"path":"https://longhaisk.github.io/HTLR/reference/htlr.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a HTLR Model — htlr","title":"Fit a HTLR Model — htlr","text":"function trains linear logistic regression models HMC restricted Gibbs sampling.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a HTLR Model — htlr","text":"","code":"htlr(   X,   y,   fsel = 1:ncol(X),   stdx = TRUE,   prior = \"t\",   df = 1,   iter = 2000,   warmup = floor(iter/2),   thin = 1,   init = \"lasso\",   leap = 50,   leap.warm = floor(leap/10),   leap.stepsize = 0.3,   cut = 0.05,   verbose = FALSE,   rep.legacy = FALSE,   keep.warmup.hist = FALSE,   ... )"},{"path":"https://longhaisk.github.io/HTLR/reference/htlr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a HTLR Model — htlr","text":"X Input matrix, dimension nobs nvars; row observation vector. y Vector response variables. Must coded non-negative integers, e.g., 1,2,...,C C classes, label 0 also allowed. fsel Subsets features selected fitting, univariate screening. stdx Logical; TRUE, original feature values standardized mean = 0 sd = 1. prior prior applied model. Either list hyperparameter settings returned htlr_prior character string \"t\" (student-t), \"ghs\" (horseshoe), \"neg\" (normal-exponential-gamma). df degree freedom t/ghs/neg prior coefficients. ignored configuration list htlr_prior passed prior. iter positive integer specifying number iterations (including warmup). warmup positive integer specifying number warmup (aka burnin). number warmup iterations larger iter default iter / 2. thin positive integer specifying period saving samples. init initial state Markov Chain; accepts three forms: previously fitted fithtlr object, user supplied initial coeficient matrix (p+1)*K, p number features, K number classes y minus 1, character string matches following: \"lasso\" - (Default) Use Lasso initial state lambda chosen   cross-validation. Users may specify candidate lambda values via   optional argument lasso.lambda. customized Lasso initial   states can generated lasso_deltas. \"bcbc\" - Use initial state generated package BCBCSF   (Bias-corrected Bayesian classification). customized BCBCSF initial   states can generated bcbcsf_deltas. WARNING: type   initial states can used continuous features gene expression profiles,   used categorical features SNP profiles. \"random\" - Use random initial values sampled N(0, 1). leap length leapfrog trajectory sampling phase. leap.warm length leapfrog trajectory burnin phase. leap.stepsize integrator step size used Hamiltonian simulation. cut coefficients smaller criteria fixed HMC updating step. verbose Logical; setting TRUE tracking MCMC sampling iterations. rep.legacy Logical; TRUE, output produced HTLR versions legacy-3.1-1 reproduced. speed typically slower non-legacy mode multi-core machine. Default FALSE. keep.warmup.hist Warmup iterations recorded default, set TRUE enable . ... optional parameters: rda.alpha - user supplied alpha value bcbcsf_deltas. Default: 0.2. lasso.lambda - user supplied lambda sequence lasso_deltas.   Default: {.01, .02, ..., .05}. ignored rep.legacy set TRUE.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a HTLR Model — htlr","text":"object S3 class htlr.fit.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a HTLR Model — htlr","text":"Longhai Li Weixin Yao (2018). Fully Bayesian Logistic Regression Hyper-Lasso Priors High-dimensional Feature Selection. Journal Statistical Computation Simulation 2018, 88:14, 2827-2851.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a HTLR Model — htlr","text":"","code":"set.seed(12345) data(\"colon\")  ## fit HTLR models with selected features, note that the chain length setting is for demo only  ## using t prior with 1 df and log-scale fixed to -10  fit.t <- htlr(X = colon$X, y = colon$y, fsel = 1:100,               prior = htlr_prior(\"t\", df = 1, logw = -10),                init = \"bcbc\", iter = 20, thin = 1)  ## using NEG prior with 1 df and log-scale fixed to -10  fit.neg <- htlr(X = colon$X, y = colon$y, fsel = 1:100,                 prior = htlr_prior(\"neg\", df = 1, logw = -10),                  init = \"bcbc\", iter = 20, thin = 1) #> Warning: random logw currently only supports t prior  ## using horseshoe prior with 1 df and auto-selected log-scale    fit.ghs <- htlr(X = colon$X, y = colon$y, fsel = 1:100,                 prior = \"ghs\", df = 1, init = \"bcbc\",                 iter = 20, thin = 1) #> Warning: random logw currently only supports t prior"},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a HTLR Model (Internal API) — htlr_fit","title":"Fit a HTLR Model (Internal API) — htlr_fit","text":"function trains linear logistic regression models HMC restricted Gibbs sampling. also makes predictions test cases X_ts provided.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a HTLR Model (Internal API) — htlr_fit","text":"","code":"htlr_fit(   X_tr,   y_tr,   fsel = 1:ncol(X_tr),   stdzx = TRUE,   ptype = c(\"t\", \"ghs\", \"neg\"),   sigmab0 = 2000,   alpha = 1,   s = -10,   eta = 0,   iters_h = 1000,   iters_rmc = 1000,   thin = 1,   leap_L = 50,   leap_L_h = 5,   leap_step = 0.3,   hmc_sgmcut = 0.05,   initial_state = \"lasso\",   keep.warmup.hist = FALSE,   silence = TRUE,   rep.legacy = TRUE,   alpha.rda = 0.2,   lasso.lambda = seq(0.05, 0.01, by = -0.01),   X_ts = NULL,   predburn = NULL,   predthin = 1 )"},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a HTLR Model (Internal API) — htlr_fit","text":"X_tr Input matrix, dimension nobs nvars; row observation vector. y_tr Vector response variables. Must coded non-negative integers, e.g., 1,2,...,C C classes, label 0 also allowed. fsel Subsets features selected fitting, univariate screening. stdzx Logical; TRUE, original feature values standardized mean = 0 sd = 1. ptype prior applied model. Either \"t\" (student-t, default), \"ghs\" (horseshoe), \"neg\" (normal-exponential-gamma). sigmab0 sd normal prior intercept. alpha degree freedom t/ghs/neg prior coefficients. s log scale priors (logw) coefficients. eta sd normal prior logw. set 0, logw fixed. Otherwise, logw assigned normal prior updated sampling. iters_h positive integer specifying number warmup (aka burnin). iters_rmc positive integer specifying number iterations warmup. thin positive integer specifying period saving samples. leap_L length leapfrog trajectory sampling phase. leap_L_h length leapfrog trajectory burnin phase. leap_step stepsize adjustment multiplied second-order partial derivatives log posterior. hmc_sgmcut coefficients smaller criteria fixed HMC updating step. initial_state initial state Markov Chain; can previously fitted fithtlr object, user supplied initial state vector, character string matches following: \"lasso\" - (Default) Use Lasso initial state lambda chosen   cross-validation. Users may specify candidate lambda values via   optional argument lasso.lambda. customized Lasso initial   states can generated lasso_deltas. \"bcbcsfrda\" - Use initial state generated package BCBCSF   (Bias-corrected Bayesian classification). customized BCBCSF initial   states can generated bcbcsf_deltas. WARNING: type   initial states can used continuous features gene expression profiles,   used categorical features SNP profiles. \"random\" - Use random initial values sampled N(0, 1). keep.warmup.hist Warmup iterations recorded default, set TRUE enable . silence Setting FALSE tracking MCMC sampling iterations. rep.legacy Logical; TRUE, output produced HTLR versions legacy-3.1-1 reproduced. speed typically slower non-legacy mode multi-core machine. alpha.rda user supplied alpha value bcbcsf_deltas setting BCBCSF initial state. Default: 0.2. lasso.lambda - user supplied lambda sequence lasso_deltas setting Lasso initial state. Default: {.01, .02, ..., .05}. ignored rep.legacy set TRUE. X_ts Test data predictions made. predburn, predthin prediction base X_ts (supplied), predburn Markov chain (super)iterations discarded, every predthin used inference.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a HTLR Model (Internal API) — htlr_fit","text":"list fitting results. X_ts provided, list object S3 class htlr.fit.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_fit.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a HTLR Model (Internal API) — htlr_fit","text":"Longhai Li Weixin Yao (2018). Fully Bayesian Logistic Regression Hyper-Lasso Priors High-dimensional Feature Selection. Journal Statistical Computation Simulation 2018, 88:14, 2827-2851.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Prediction on New Data (Advanced) — htlr_predict","title":"Make Prediction on New Data (Advanced) — htlr_predict","text":"function uses MCMC samples fitted htlrfit object user supplied regression coefficient predict class labels test cases.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Prediction on New Data (Advanced) — htlr_predict","text":"","code":"htlr_predict(   X_ts,   fithtlr = NULL,   deltas = NULL,   burn = NULL,   thin = 1,   usedmc = NULL,   rep.legacy = TRUE )"},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Prediction on New Data (Advanced) — htlr_predict","text":"X_ts Matrix values predictions made. fithtlr Fitted HTLR model object. deltas values deltas (example true deltas) used make prediction; override fithtlr provided. burn, thin burn Markov chain (super)iterations discarded prediction, every thin used. usedmc Indices Markov chain iterations used inference. supplied, burn thin ignored. rep.legacy reproduce (actually incorrect) results legacy version. See https://github.com/longhaiSK/HTLR/issues/7.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Prediction on New Data (Advanced) — htlr_predict","text":"matrix predictive probabilities, rows cases, cols classes.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Prior Configuration — htlr_prior","title":"Generate Prior Configuration — htlr_prior","text":"Configure prior hyper-parameters HTLR model fitting","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Prior Configuration — htlr_prior","text":"","code":"htlr_prior(   ptype = c(\"t\", \"ghs\", \"neg\"),   df = 1,   logw = -(1/df) * 10,   eta = ifelse(df > 1, 3, 0),   sigmab0 = 2000 )"},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Prior Configuration — htlr_prior","text":"ptype prior applied model. Either \"t\" (student-t, default), \"ghs\" (horseshoe), \"neg\" (normal-exponential-gamma). df degree freedom (aka alpha) t/ghs/neg prior coefficients. logw log scale priors coefficients. eta sd normal prior logw. set 0, logw fixed. Otherwise, logw assigned normal prior updated sampling. sigmab0 sd normal prior intercept.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Prior Configuration — htlr_prior","text":"configuration list containing ptype, alpha, logw, eta, sigmab0.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_prior.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Prior Configuration — htlr_prior","text":"output configuration list passed prior argument htlr. naive users, need specify prior type degree freedom, hyper-parameters chosen automatically. advanced users, can supply prior hyper-parameters . suggestion picking hyper-parameters, see references.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/htlr_prior.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Prior Configuration — htlr_prior","text":"Longhai Li Weixin Yao. (2018). Fully Bayesian Logistic Regression Hyper-Lasso Priors High-dimensional Feature Selection. Journal Statistical Computation Simulation 2018, 88:14, 2827-2851.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/lasso_deltas.html","id":null,"dir":"Reference","previous_headings":"","what":"Lasso Initial State — lasso_deltas","title":"Lasso Initial State — lasso_deltas","text":"Generate initial Markov chain state Lasso.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/lasso_deltas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lasso Initial State — lasso_deltas","text":"","code":"lasso_deltas(   X,   y,   lambda = NULL,   verbose = FALSE,   alpha = 1,   rank_fn = order_plain,   k = ncol(X) )"},{"path":"https://longhaisk.github.io/HTLR/reference/lasso_deltas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lasso Initial State — lasso_deltas","text":"X Design matrix traning data; rows cases, columns different features. y Vector class labels training test data set. Must coded non-negative integers, e.g., 1,2,...,C C classes. lambda user supplied lambda sequence glmnet cross-validation. NULL default, generated glmnet. alpha elasticnet mixing parameter glmnet.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/lasso_deltas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lasso Initial State — lasso_deltas","text":"matrix - initial state Markov Chain HTLR model fitting.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/lasso_deltas.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Lasso Initial State — lasso_deltas","text":"Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1-22.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/nzero_idx.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Indices of Non-Zero Coefficients — nzero_idx","title":"Get Indices of Non-Zero Coefficients — nzero_idx","text":"Get indices non-zero coefficients fitted HTLR model objects.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/nzero_idx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Indices of Non-Zero Coefficients — nzero_idx","text":"","code":"nzero_idx(fit, cut = 0.1)"},{"path":"https://longhaisk.github.io/HTLR/reference/nzero_idx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Indices of Non-Zero Coefficients — nzero_idx","text":"fit object S3 class htlr.fit. cut Threshold relative SDB distinguish zero coefficients.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/nzero_idx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Indices of Non-Zero Coefficients — nzero_idx","text":"Indices vector non-zero coefficients model.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/nzero_idx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Indices of Non-Zero Coefficients — nzero_idx","text":"","code":"set.seed(12345) data(\"colon\")  fit <- htlr(X = colon$X, y = colon$y, fsel = 1:100, iter = 20) nzero_idx(fit) #> [1] 14 43"},{"path":"https://longhaisk.github.io/HTLR/reference/order_ftest.html","id":null,"dir":"Reference","previous_headings":"","what":"Order features by F-statistic — order_ftest","title":"Order features by F-statistic — order_ftest","text":"function orders features terms ANOVA F-statistic.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_ftest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Order features by F-statistic — order_ftest","text":"","code":"order_ftest(X, y)"},{"path":"https://longhaisk.github.io/HTLR/reference/order_ftest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Order features by F-statistic — order_ftest","text":"X Input matrix, dimension nobs nvars; row observation vector. y Vector response variables.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_ftest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Order features by F-statistic — order_ftest","text":"Order features length nvars.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_ftest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Order features by F-statistic — order_ftest","text":"","code":"data(\"diabetes392\") order_ftest(diabetes392$X, diabetes392$y) #> [1] 2 8 5 6 1 4 7 3"},{"path":"https://longhaisk.github.io/HTLR/reference/order_kruskal.html","id":null,"dir":"Reference","previous_headings":"","what":"Order features by Kruskal-Wallis test — order_kruskal","title":"Order features by Kruskal-Wallis test — order_kruskal","text":"function orders features terms Kruskal-Wallis test p-value.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_kruskal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Order features by Kruskal-Wallis test — order_kruskal","text":"","code":"order_kruskal(X, y)"},{"path":"https://longhaisk.github.io/HTLR/reference/order_kruskal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Order features by Kruskal-Wallis test — order_kruskal","text":"X Input matrix, dimension nobs nvars; row observation vector. y Vector response variables.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_kruskal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Order features by Kruskal-Wallis test — order_kruskal","text":"Order features length nvars.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_kruskal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Order features by Kruskal-Wallis test — order_kruskal","text":"","code":"data(\"diabetes392\") order_kruskal(diabetes392$X, diabetes392$y) #> [1] 2 8 5 6 4 1 7 3"},{"path":"https://longhaisk.github.io/HTLR/reference/order_plain.html","id":null,"dir":"Reference","previous_headings":"","what":"Plain order function — order_plain","title":"Plain order function — order_plain","text":"placeholder order function returns original order given features.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_plain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plain order function — order_plain","text":"","code":"order_plain(X, y)"},{"path":"https://longhaisk.github.io/HTLR/reference/order_plain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plain order function — order_plain","text":"X Input matrix, dimension nobs nvars; row observation vector. y Vector response variables.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/order_plain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plain order function — order_plain","text":"Sequence starting 1 nvars.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://longhaisk.github.io/HTLR/reference/predict.htlr.fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Prediction on New Data — predict.htlr.fit","title":"Make Prediction on New Data — predict.htlr.fit","text":"Similar predict methods, function returns predictions fitted htlrfit object.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/predict.htlr.fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Prediction on New Data — predict.htlr.fit","text":"","code":"# S3 method for class 'htlr.fit' predict(object, newx, type = c(\"response\", \"class\"), ...)"},{"path":"https://longhaisk.github.io/HTLR/reference/predict.htlr.fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Prediction on New Data — predict.htlr.fit","text":"object fitted model object S3 class htlrfit. newx Matrix values predictions made. type Type prediction required. Type \"response\" gives fitted probabilities. Type \"class\" produces class label corresponding maximum probability. ... Advanced options specify Markov chain iterations used inference. See htlr_predict.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/predict.htlr.fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Prediction on New Data — predict.htlr.fit","text":"object returned depends type.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/split_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Split Data into Train and Test Partitions — split_data","title":"Split Data into Train and Test Partitions — split_data","text":"function splits input data response variables training testing parts.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/split_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split Data into Train and Test Partitions — split_data","text":"","code":"split_data(X, y, p.train = 0.7, n.train = round(nrow(X) * p.train))"},{"path":"https://longhaisk.github.io/HTLR/reference/split_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split Data into Train and Test Partitions — split_data","text":"X Input matrix, dimension nobs nvars; row observation vector. y Vector response variables. p.train Percentage training set. n.train Number cases training; override p.train specified.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/split_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split Data into Train and Test Partitions — split_data","text":"List training data x.tr, y.tr testing data x.te, y.te.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/split_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split Data into Train and Test Partitions — split_data","text":"","code":"dat <- gendata_MLR(n = 100, p = 10) dat <- split_data(dat$X, dat$y, p.train = 0.7) dim(dat$x.tr) #> [1] 70 10 dim(dat$x.te) #> [1] 30 10"},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":null,"dir":"Reference","previous_headings":"","what":"Standardizes a Design Matrix — std","title":"Standardizes a Design Matrix — std","text":"function accepts design matrix returns standardized version matrix, statistics column median sd also provided.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standardizes a Design Matrix — std","text":"","code":"std(X, tol = 1e-06)"},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standardizes a Design Matrix — std","text":"X Design matrix, dimension nobs nvars; row observation vector; can also object can coerced matrix, e.g. data.frame. tol tolerance value; column X considered singular sd entries (observations) less tol. Singular columns dropped end.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standardizes a Design Matrix — std","text":"standardized design matrix following attributes: nonsingular Indices non-singular columns. center Median non-singular column used standardization. scale Standard deviation non-singular column used standardization.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Standardizes a Design Matrix — std","text":"column X, standardization done first subtracting median, dividing sample standard deviation, original version ncvreg uses mean population standard deviation. speed slower ncvreg complexity median finding, still substantially faster scale() provided R base.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Standardizes a Design Matrix — std","text":"Patrick Breheny (original)  Steven Liu (modification)","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/std.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Standardizes a Design Matrix — std","text":"","code":"set.seed(123) mat <- matrix(rnorm(n = 80 * 90, mean = 100, sd = 50), 80, 90) mat %>% as.numeric() %>% ggplot2::qplot(bins = 30, xlab = '')  mat %>% std() %>% as.numeric() %>% ggplot2::qplot(bins = 30, xlab = '')"},{"path":"https://longhaisk.github.io/HTLR/reference/summary.htlr.fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior Summaries — summary.htlr.fit","title":"Posterior Summaries — summary.htlr.fit","text":"function gives summary posterior parameters.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/summary.htlr.fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior Summaries — summary.htlr.fit","text":"","code":"# S3 method for class 'htlr.fit' summary(   object,   features = 1L:object$p,   method = median,   usedmc = get_sample_indice(dim(object$mcdeltas)[3], object$mc.param$iter.rmc),   ... )"},{"path":"https://longhaisk.github.io/HTLR/reference/summary.htlr.fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior Summaries — summary.htlr.fit","text":"object object S3 class htlr.fit. features vector indices (int) names (char) specify parameters look . default parameters selected. method function used aggregate MCMC samples. default median, built-/customized statistical functions mean, sd, mad can also used. usedmc Indices Markov chain iterations used inference. default iterations used. ... used.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/summary.htlr.fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior Summaries — summary.htlr.fit","text":"point summary MCMC samples.","code":""},{"path":"https://longhaisk.github.io/HTLR/reference/summary.htlr.fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior Summaries — summary.htlr.fit","text":"","code":"set.seed(12345) data(\"colon\")  fit <- htlr(X = colon$X, y = colon$y, fsel = 1:100, iter = 20) summary(fit, features = 1:16) #>                class 1 #> Intercept  0.938177120 #> V1         0.000000000 #> V2         0.000000000 #> V3         0.000000000 #> V4         0.000000000 #> V5         0.000000000 #> V6         0.000000000 #> V7         0.000000000 #> V8         0.000000000 #> V9         0.000000000 #> V10        0.000000000 #> V11        0.000000000 #> V12        0.000000000 #> V13        0.000000000 #> V14       -2.045512555 #> V15        0.002115235 #> V16       -0.019430491 #> attr(,\"stats\") #> [1] \"median\""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"htlr-04-3","dir":"Changelog","previous_headings":"","what":"HTLR 0.4-3","title":"HTLR 0.4-3","text":"CRAN release: 2020-09-09","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"new-features-0-4-3","dir":"Changelog","previous_headings":"","what":"New Features","title":"HTLR 0.4-3","text":"Added option users keep samples warmup iterations.","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"improvements-0-4-3","dir":"Changelog","previous_headings":"","what":"Improvements","title":"HTLR 0.4-3","text":"Bug fix [#7].","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"htlr-04-2","dir":"Changelog","previous_headings":"","what":"HTLR 0.4-2","title":"HTLR 0.4-2","text":"CRAN release: 2020-01-17","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"new-features-0-4-2","dir":"Changelog","previous_headings":"","what":"New Features","title":"HTLR 0.4-2","text":"Added new function std() feature standardization. Added new dataset diabetes392.","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"improvements-0-4-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"HTLR 0.4-2","text":"htlr() predict.htlr.fit() now handles non-matrix input, .e. data.frame. Minor speed improvement htlr() gendata_FAM(). Updated documentation htlr().","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"note-0-4-2","dir":"Changelog","previous_headings":"","what":"Note","title":"HTLR 0.4-2","text":"Changed package license GPLv2 GPLv3.","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"htlr-04-1","dir":"Changelog","previous_headings":"","what":"HTLR 0.4-1","title":"HTLR 0.4-1","text":"CRAN release: 2019-10-08","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"bug-fixes-0-4-1","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"HTLR 0.4-1","text":"Fixed potential memory leak issue ARS module.","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"htlr-04","dir":"Changelog","previous_headings":"","what":"HTLR 0.4","title":"HTLR 0.4","text":"CRAN release: 2019-10-06","code":""},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"new-features-0-4","dir":"Changelog","previous_headings":"","what":"New Features","title":"HTLR 0.4","text":"first released version revamped HTLR. Gibbs sampling routine completely refactored using RcppArmadillo, leads significant performance gain multi-core/distributed machines. fitted model object registered S3 class htlrfit, coming set useful S3 methods print(), summary(), predict(), .matrix(), nobs(). New model fitting function htlr() accessible interface, htlr_fit() htlr_predict() still keeped best possible backward compatibility. Better cohesion bayesplot packages RStan toolchain. Added new dataset colon.","code":""},{"path":[]},{"path":"https://longhaisk.github.io/HTLR/news/index.html","id":"note-0-3","dir":"Changelog","previous_headings":"","what":"Note","title":"HTLR 0.3","text":"HTLR originally created Longhai Li legacy version number 3.1-1. compatible macOS.","code":""}]
